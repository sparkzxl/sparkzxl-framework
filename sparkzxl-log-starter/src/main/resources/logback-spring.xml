<?xml version="1.0" encoding="UTF-8"?>
<configuration debug="false" scan="true" scanPeriod="1 minutes">
    <include resource="org/springframework/boot/logging/logback/defaults.xml"/>

    <!-- 应用名称 -->
    <springProperty name="applicationName" scope="context" source="spring.application.name" defaultValue="application"/>
    <!-- 应用名称 -->
    <springProperty name="environment" scope="context" source="spring.profiles.active" defaultValue="dev"/>
    <!-- 日志文件保存路径 -->
    <springProperty name="logHome" scope="context" source="logging.file.path" defaultValue="${user.home}/logs"/>

    <!-- 日志显隐控制方式 -->
    <springProperty name="enableConsole" scope="context" source="logging.enable-console" defaultValue="true"/>
    <springProperty name="enableJson" scope="context" source="logging.file.enable-json" defaultValue="false"/>
    <springProperty name="enableAlarm" scope="context" source="logging.alarm.enabled" defaultValue="false"/>

    <springProperty name="enableFile" scope="context" source="logging.file.enable" defaultValue="false"/>
    <springProperty name="maxHistory" scope="context" source="logging.file.maxHistory" defaultValue="7"/>
    <springProperty name="maxFileSize" scope="context" source="logging.file.maxFileSize" defaultValue="10MB"/>
    <springProperty name="totalSizeCap" scope="context" source="logging.file.totalSizeCap" defaultValue="10GB"/>

    <!-- kafka 配置 -->
    <springProperty name="enableKafka" scope="context" source="logging.kafka.enable" defaultValue="false"/>
    <springProperty name="kafka.servers" scope="context" source="logging.kafka.servers"/>
    <springProperty name="kafka.topic" scope="context" source="logging.kafka.topic"/>
    <springProperty name="kafka.linger" scope="context" source="logging.kafka.linger" defaultValue="1000"/>
    <springProperty name="kafka.acks" scope="context" source="logging.kafka.acks" defaultValue="0"/>
    <springProperty name="kafka.maxBlock" scope="context" source="logging.kafka.maxBlock" defaultValue="0"/>

    <springProperty scope="context" name="log.level.console" source="logging.level.console" defaultValue="INFO"/>

    <!-- 控制台 日志格式化 -->
    <property name="CONSOLE_LOG_PATTERN"
              value="%clr(%d{${LOG_DATEFORMAT_PATTERN:-yyyy-MM-dd HH:mm:ss.SSS}}){faint} %blue(environment): %yellow(${environment}) %blue(application): %yellow(${applicationName}) %highlight(${LOG_LEVEL_PATTERN:-%5p}) %clr(${PID:- }){magenta} [%X{tid}] %clr(---){faint} %clr([%15.15t]){faint} %clr(%-40.40logger{15}){boldCyan} %clr(:){faint} %m%n${LOG_EXCEPTION_CONVERSION_WORD:-%wEx}"/>

    <property name="FILE_LOG_PATTERN"
              value="%d{${LOG_DATEFORMAT_PATTERN:-yyyy-MM-dd HH:mm:ss.SSS}} environment: ${environment} application: ${applicationName} ${LOG_LEVEL_PATTERN:-%5p} ${PID:- } [%X{tid}] [%-40.40logger{15}] : %m%n${LOG_EXCEPTION_CONVERSION_WORD:-%wEx}"/>

    <!-- 控制台 Appender -->
    <appender name="ConsoleOutput" class="ch.qos.logback.core.ConsoleAppender">
        <withJansi>true</withJansi>
        <!-- 日志的格式化 -->
        <encoder class="ch.qos.logback.core.encoder.LayoutWrappingEncoder">
            <layout class="org.apache.skywalking.apm.toolkit.log.logback.v1.x.mdc.TraceIdMDCPatternLogbackLayout">
                <pattern>${CONSOLE_LOG_PATTERN}</pattern>
            </layout>
            <charset>utf8</charset>
        </encoder>
    </appender>

    <!--每天记录日志到文件appender-->
    <appender name="FileOutput" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <file>${logHome}/${applicationName}.log</file>
        <immediateFlush>false</immediateFlush>
        <rollingPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy">
            <fileNamePattern>${logHome}/${applicationName}-%d{yyyy-MM-dd}.%i.gz</fileNamePattern>
            <maxHistory>${maxHistory}</maxHistory>
            <maxFileSize>${maxFileSize}</maxFileSize>
            <totalSizeCap>${totalSizeCap}</totalSizeCap>
        </rollingPolicy>
        <if condition='property("enableJson").contains("true")'>
            <then>
                <encoder charset="UTF-8" class="net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder">
                    <providers class="net.logstash.logback.composite.loggingevent.LoggingEventJsonProviders">
                        <pattern>
                            <pattern>
                                {"date": "%d{yyyy-MM-dd HH:mm:ss.SSS}",
                                "level": "%level",
                                "environment": "${environment}",
                                "application": "${applicationName}",
                                "tid": "%X{tid}",
                                "pid": "${PID:-}",
                                "thread": "%thread",
                                "class": "%logger{15}",
                                "message": "%message"}
                            </pattern>
                        </pattern>
                    </providers>
                </encoder>
            </then>
            <else>
                <!-- 日志的格式化 -->
                <encoder>
                    <pattern>${FILE_LOG_PATTERN}</pattern>
                    <charset>utf8</charset>
                </encoder>
            </else>
        </if>
    </appender>

    <!-- 日志信息发送kafka -->
    <appender name="kafkaAppender" class="com.github.danielwegener.logback.kafka.KafkaAppender">
        <encoder>
            <pattern>
                {"date": "%d{yyyy-MM-dd HH:mm:ss.SSS}",
                "level": "%level",
                "environment": "${environment}",
                "application": "${applicationName}",
                "tid": "%X{tid}",
                "pid": "${PID:-}",
                "thread": "%thread",
                "class": "%logger{40}",
                "message": "%message"}
            </pattern>
            <charset>utf8</charset>
        </encoder>
        <!-- kafka topic -->
        <topic>${kafka.topic}</topic>
        <keyingStrategy class="com.github.danielwegener.logback.kafka.keying.NoKeyKeyingStrategy"/>
        <deliveryStrategy class="com.github.danielwegener.logback.kafka.delivery.AsynchronousDeliveryStrategy"/>
        <!-- kafka 地址 -->
        <producerConfig>bootstrap.servers=${kafka.servers}</producerConfig>
        <!-- ack确认机制  -->
        <producerConfig>acks=${kafka.acks}</producerConfig>
        <!-- 等待最多1000 毫秒并收集日志消息，然后将它们作为批处理发送 -->
        <producerConfig>linger.ms=${kafka.linger}</producerConfig>
        <!-- 即使生产者缓冲区已满，也不要阻塞应用程序而是开始丢弃消息 -->
        <producerConfig>max.block.ms=${kafka.max.block}</producerConfig>
        <!-- 定义一个客户端 ID，用于在 kafka broker 中识别自己 -->
        <producerConfig>client.id=${HOSTNAME}-${CONTEXT_NAME}-logback-relaxed</producerConfig>
    </appender>

    <!-- skywalking grpc日志传输-->
    <appender name="grpc-log" class="org.apache.skywalking.apm.toolkit.log.logback.v1.x.log.GRPCLogClientAppender">
        <encoder class="ch.qos.logback.core.encoder.LayoutWrappingEncoder">
            <layout class="org.apache.skywalking.apm.toolkit.log.logback.v1.x.mdc.TraceIdMDCPatternLogbackLayout">
                <pattern>%m%n${LOG_EXCEPTION_CONVERSION_WORD:-%wEx}</pattern>
            </layout>
        </encoder>
    </appender>

    <!-- 异步控制台打印 -->
    <appender name="AsyncConsole" class="ch.qos.logback.classic.AsyncAppender">
        <discardingThreshold>0</discardingThreshold>
        <queueSize>100</queueSize>
        <neverBlock>true</neverBlock>
        <appender-ref ref="ConsoleOutput"/>
    </appender>

    <!-- 异步输出文件 -->
    <appender name="AsyncFile" class="ch.qos.logback.classic.AsyncAppender">
        <discardingThreshold>0</discardingThreshold>
        <queueSize>100</queueSize>
        <neverBlock>true</neverBlock>
        <appender-ref ref="FileOutput"/>
    </appender>

    <!-- 异步传递策略，建议选择异步，不然连接kafka失败，会阻挡服务启动 -->
    <appender name="AsyncKafka" class="ch.qos.logback.classic.AsyncAppender">
        <discardingThreshold>0</discardingThreshold>
        <queueSize>4096</queueSize>
        <neverBlock>true</neverBlock>
        <includeCallerData>true</includeCallerData>
        <appender-ref ref="kafkaAppender"/>
    </appender>


    <appender name="AlarmLog" class="com.github.sparkzxl.log.AlarmLogLogbackAsyncAppender">
        <appender-ref ref="ConsoleOutput"/>
    </appender>

    <logger name="springfox.documentation" level="ERROR"/>
    <logger name="com.alibaba.nacos" level="ERROR"/>

    <root level="${log.level.console}">
        <appender-ref ref="grpc-log"/>
        <if condition='property("enableFile").contains("true")'>
            <then>
                <appender-ref ref="AsyncFile"/>
            </then>
        </if>
        <if condition='property("enableConsole").contains("true")'>
            <then>
                <appender-ref ref="AsyncConsole"/>
            </then>
        </if>
        <if condition='property("enableKafka").contains("true")'>
            <then>
                <appender-ref ref="AsyncKafka"/>
            </then>
        </if>
        <if condition='property("enableAlarm").contains("true")'>
            <then>
                <appender-ref ref="AlarmLog"/>
            </then>
        </if>
    </root>
</configuration>
